# Rozdział 2: Uczenie statystyczne

## 2.2 Ocena dokładności modelu

Jednym z kluczowych celów tej książki jest zapoznanie czytelnika z szerokim zakresem metod uczenia statystycznego, które wykraczają daleko poza standardowe podejście regresji liniowej. Dlaczego konieczne jest wprowadzanie tak wielu różnych podejść do uczenia statystycznego, zamiast tylko jednej, najlepszej metody? W statystyce nie ma darmowych obiadów: żadna pojedyncza metoda nie dominuje nad wszystkimi innymi we wszystkich możliwych zbiorach danych. W przypadku konkretnego zbioru danych jedna metoda może działać najlepiej, ale inna metoda może działać lepiej na podobnym, lecz innym zbiorze danych. Dlatego ważnym zadaniem jest zdecydowanie dla danego zbioru danych, która metoda daje najlepsze wyniki. Wybór najlepszego podejścia może być jednym z najtrudniejszych zadań w praktycznym stosowaniu uczenia statystycznego.

---

### 2.2.1 Pomiar jakości dopasowania

Aby ocenić wydajność metody uczenia statystycznego na danym zbiorze danych, potrzebujemy sposobu na zmierzenie, jak dobrze jej prognozy faktycznie pasują do obserwowanych danych. Oznacza to, że musimy określić ilościowo, w jakim stopniu przewidywana wartość odpowiedzi dla danej obserwacji jest bliska prawdziwej wartości odpowiedzi dla tej obserwacji. W przypadku regresji najczęściej używaną miarą jest **błąd średniokwadratowy (MSE)**, dany wzorem

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{f}(x_{i}))^{2}, \quad (2.5)$$

gdzie $\hat{f}(x_{i})$ jest prognozą, jaką $\hat{f}$ daje dla i-tej obserwacji. MSE będzie mały, jeśli przewidywane odpowiedzi są bardzo bliskie prawdziwym odpowiedziom, i będzie duży, jeśli dla niektórych obserwacji przewidywane i prawdziwe odpowiedzi znacznie się różnią.

MSE w (2.5) jest obliczany przy użyciu danych treningowych, które zostały użyte do dopasowania modelu, dlatego dokładniej powinien być nazywany **treningowym MSE**. Ale ogólnie rzecz biorąc, nie zależy nam na tym, jak dobrze metoda działa na danych treningowych. Interesuje nas raczej dokładność prognoz, które uzyskujemy, gdy stosujemy naszą metodę do wcześniej niewidzianych **danych testowych**. Dlaczego to jest dla nas ważne? Załóżmy, że interesuje nas opracowanie algorytmu do przewidywania ceny akcji na podstawie poprzednich zwrotów z akcji. Możemy trenować metodę, używając zwrotów z akcji z ostatnich 6 miesięcy. Ale tak naprawdę nie zależy nam na tym, jak dobrze nasza metoda przewiduje cenę akcji z zeszłego tygodnia. Zależy nam na tym, jak dobrze przewidzi cenę jutro lub w przyszłym miesiącu. Podobnie, załóżmy, że mamy pomiary kliniczne (np. wagę, ciśnienie krwi, wzrost, wiek, historię chorób w rodzinie) dla wielu pacjentów, a także informację, czy każdy z pacjentów ma cukrzycę. Możemy użyć tych pacjentów do wytrenowania metody uczenia statystycznego do przewidywania ryzyka cukrzycy na podstawie pomiarów klinicznych. W praktyce chcemy, aby ta metoda dokładnie przewidywała ryzyko cukrzycy u przyszłych pacjentów na podstawie ich pomiarów klinicznych. Niewiele nas interesuje, czy metoda dokładnie przewiduje ryzyko cukrzycy u pacjentów użytych do trenowania modelu, ponieważ już wiemy, którzy z tych pacjentów mają cukrzycę.

Mówiąc bardziej matematycznie, załóżmy, że dopasowujemy naszą metodę uczenia statystycznego do naszych obserwacji treningowych $\{(x_{1}, y_{1}), (x_{2}, y_{2}),..., (x_{n}, y_{n})\}$ i uzyskujemy estymację $\hat{f}$. Możemy wtedy obliczyć $\hat{f}(x_{1}), \hat{f}(x_{2}),..., \hat{f}(x_{n})$. Jeśli są one w przybliżeniu równe $y_{1}, y_{2},..., y_{n}$, to treningowy MSE dany wzorem (2.5) jest mały. Jednak tak naprawdę nie interesuje nas, czy $\hat{f}(x_{i}) \approx y_{i}$; zamiast tego chcemy wiedzieć, czy $\hat{f}(x_{0})$ jest w przybliżeniu równe $y_{0}$, gdzie $(x_{0}, y_{0})$ jest wcześniej niewidzianą **obserwacją testową**, która nie została użyta do trenowania metody uczenia statystycznego. Chcemy wybrać metodę, która daje najniższy **testowy MSE**, w przeciwieństwie do najniższego treningowego MSE. Innymi słowy, gdybyśmy mieli dużą liczbę obserwacji testowych, moglibyśmy obliczyć

$$Ave(y_{0}-\hat{f}(x_{0}))^{2} \quad (2.6)$$

średni błąd kwadratowy predykcji dla tych obserwacji testowych $(x_{0}, y_{0})$. Chcielibyśmy wybrać model, dla którego ta wielkość jest jak najmniejsza.

Jak możemy próbować wybrać metodę, która minimalizuje testowy MSE? W niektórych sytuacjach możemy mieć dostępny zbiór danych testowych — to znaczy, możemy mieć dostęp do zbioru obserwacji, które nie zostały użyte do trenowania metody uczenia statystycznego. Możemy wtedy po prostu ocenić (2.6) na obserwacjach testowych i wybrać metodę uczenia, dla której testowy MSE jest najmniejszy. Ale co, jeśli nie ma dostępnych obserwacji testowych? W takim przypadku można by pomyśleć o wybraniu metody uczenia statystycznego, która minimalizuje treningowy MSE (2.5). Wydaje się to sensownym podejściem, ponieważ treningowy MSE i testowy MSE wydają się być ze sobą blisko związane. Niestety, z tą strategią wiąże się fundamentalny problem: nie ma gwarancji, że metoda z najniższym treningowym MSE będzie miała również najniższy testowy MSE. Mówiąc ogólnie, problem polega na tym, że wiele metod statystycznych specjalnie estymuje współczynniki tak, aby zminimalizować MSE zbioru treningowego. Dla tych metod, MSE zbioru treningowego może być dość mały, ale testowy MSE jest często znacznie większy.

Rysunek 2.9 ilustruje to zjawisko na prostym przykładzie. W lewym panelu Rysunku 2.9 wygenerowaliśmy obserwacje z (2.1) z prawdziwym f danym przez czarną krzywą. Krzywe pomarańczowa, niebieska i zielona ilustrują trzy możliwe estymacje f uzyskane za pomocą metod o rosnącym poziomie elastyczności. Linia pomarańczowa to dopasowanie regresji liniowej, które jest stosunkowo nieelastyczne. Krzywe niebieska i zielona zostały wygenerowane przy użyciu **splineów wygładzających**, omówionych w Rozdziale 7, z różnymi poziomami gładkości. Jest jasne, że w miarę wzrostu poziomu elastyczności, krzywe coraz lepiej dopasowują się do obserwowanych danych. Zielona krzywa jest najbardziej elastyczna i bardzo dobrze pasuje do danych; jednak zauważamy, że słabo pasuje do prawdziwego f (pokazanego na czarno), ponieważ jest zbyt pofalowana. Dostosowując poziom elastyczności dopasowania splajnem wygładzającym, możemy uzyskać wiele różnych dopasowań do tych danych.

Przejdźmy teraz do prawego panelu Rysunku 2.9. Szara krzywa pokazuje średni treningowy MSE jako funkcję elastyczności, lub formalniej **stopni swobody**, dla wielu splajnów wygładzających. Stopnie swobody to wielkość, która podsumowuje elastyczność krzywej; zostanie to omówione bardziej szczegółowo w Rozdziale 7. Kwadraty pomarańczowy, niebieski i zielony wskazują MSE związane z odpowiednimi krzywymi w lewym panelu. Bardziej ograniczona i tym samym gładsza krzywa ma mniej stopni swobody niż krzywa pofalowana — zauważ, że na Rysunku 2.9 regresja liniowa jest na najbardziej restrykcyjnym końcu, z dwoma stopniami swobody. Treningowy MSE maleje monotonicznie wraz ze wzrostem elastyczności. W tym przykładzie prawdziwe f jest nieliniowe, więc pomarańczowe dopasowanie liniowe nie jest wystarczająco elastyczne, aby dobrze oszacować f. Zielona krzywa ma najniższy treningowy MSE spośród wszystkich trzech metod, ponieważ odpowiada najbardziej elastycznej z trzech krzywych dopasowanych w lewym panelu.

W tym przykładzie znamy prawdziwą funkcję f, więc możemy również obliczyć testowy MSE na bardzo dużym zbiorze testowym, jako funkcję elastyczności. (Oczywiście, ogólnie f jest nieznane, więc nie będzie to możliwe.) Testowy MSE jest przedstawiony za pomocą czerwonej krzywej w prawym panelu Rysunku 2.9. Podobnie jak w przypadku treningowego MSE, testowy MSE początkowo maleje wraz ze wzrostem poziomu elastyczności. Jednak w pewnym momencie testowy MSE wyrównuje się, a następnie zaczyna ponownie rosnąć. W konsekwencji, krzywe pomarańczowa i zielona mają wysoki testowy MSE. Niebieska krzywa minimalizuje testowy MSE, co nie powinno być zaskakujące, biorąc pod uwagę, że wizualnie wydaje się najlepiej szacować f w lewym panelu Rysunku 2.9. Pozioma linia przerywana wskazuje $Var(\epsilon)$, błąd nieredukowalny z (2.3), który odpowiada najniższemu osiągalnemu testowemu MSE spośród wszystkich możliwych metod. Stąd splajn wygładzający reprezentowany przez niebieską krzywą jest bliski optymalnemu.

W prawym panelu Rysunku 2.9, wraz ze wzrostem elastyczności metody uczenia statystycznego, obserwujemy monotoniczny spadek treningowego MSE i kształt litery U w testowym MSE. Jest to fundamentalna właściwość uczenia statystycznego, która obowiązuje niezależnie od konkretnego zbioru danych i używanej metody statystycznej. W miarę wzrostu elastyczności modelu, treningowy MSE będzie maleć, ale testowy MSE niekoniecznie. Kiedy dana metoda daje mały treningowy MSE, ale duży testowy MSE, mówimy o **przedopasowaniu** danych. Dzieje się tak, ponieważ nasza procedura uczenia statystycznego zbyt intensywnie szuka wzorców w danych treningowych i może wychwytywać wzorce, które są spowodowane jedynie losową szansą, a nie prawdziwymi właściwościami nieznanej funkcji f. Kiedy przedopasujemy dane treningowe, testowy MSE będzie bardzo duży, ponieważ domniemane wzorce, które metoda znalazła w danych treningowych, po prostu nie istnieją w danych testowych. Zauważ, że niezależnie od tego, czy wystąpiło przedopasowanie, czy nie, prawie zawsze spodziewamy się, że treningowy MSE będzie mniejszy niż testowy MSE, ponieważ większość metod uczenia statystycznego dąży bezpośrednio lub pośrednio do minimalizacji treningowego MSE. Przedopasowanie odnosi się konkretnie do przypadku, w którym mniej elastyczny model dałby mniejszy testowy MSE.

Rysunek 2.10 przedstawia kolejny przykład, w którym prawdziwe $f$ jest w przybliżeniu liniowe. Ponownie obserwujemy, że treningowy MSE maleje monotonicznie wraz ze wzrostem elastyczności modelu i że występuje kształt litery U w testowym MSE. Jednakże, ponieważ prawda jest bliska liniowości, testowy MSE tylko nieznacznie maleje, zanim ponownie zacznie rosnąć, tak że pomarańczowe dopasowanie metodą najmniejszych kwadratów jest znacznie lepsze niż wysoce elastyczna zielona krzywa. Na koniec, Rysunek 2.11 przedstawia przykład, w którym $f$ jest wysoce nieliniowe. Krzywe treningowego i testowego MSE wciąż wykazują te same ogólne wzorce, ale teraz następuje gwałtowny spadek obu krzywych, zanim testowy MSE zacznie powoli rosnąć.

W praktyce zazwyczaj można stosunkowo łatwo obliczyć treningowy MSE, ale oszacowanie testowego MSE jest znacznie trudniejsze, ponieważ zwykle nie ma dostępnych danych testowych. Jak ilustrują trzy poprzednie przykłady, poziom elastyczności odpowiadający modelowi z minimalnym testowym MSE może się znacznie różnić w zależności od zbioru danych. W tej książce omawiamy różnorodne podejścia, które można stosować w praktyce do oszacowania tego minimalnego punktu. Jedną z ważnych metod jest **walidacja krzyżowa** (Rozdział 5), która jest metodą szacowania testowego MSE przy użyciu danych treningowych.

---

### 2.2.2 Kompromis między obciążeniem a wariancją

Kształt litery U obserwowany na krzywych testowego MSE (Rysunki 2.9–2.11) okazuje się być wynikiem dwóch konkurencyjnych właściwości metod uczenia statystycznego. Chociaż dowód matematyczny wykracza poza zakres tej książki, można wykazać, że oczekiwany testowy MSE, dla danej wartości $x_{0}$, zawsze można rozłożyć na sumę trzech fundamentalnych wielkości: **wariancji** $\hat{f}(x_{0})$, kwadratu **obciążenia** $\hat{f}(x_{0})$ i wariancji błędu losowego $\epsilon$. To jest,

$$E(y_{0}-\hat{f}(x_{0}))^{2} = Var(\hat{f}(x_{0})) + [Obciążenie(\hat{f}(x_{0}))]^{2} + Var(\epsilon). \quad (2.7)$$

Tutaj notacja $E(y_{0}-\hat{f}(x_{0}))^{2}$ definiuje **oczekiwany testowy MSE** w punkcie $x_{0}$ i odnosi się do średniego testowego MSE, który uzyskalibyśmy, gdybyśmy wielokrotnie estymowali f, używając dużej liczby zbiorów treningowych, i testowali każdy w punkcie $x_{0}$. Ogólny oczekiwany testowy MSE można obliczyć, uśredniając $E(y_{0}-\hat{f}(x_{0}))^{2}$ po wszystkich możliwych wartościach $x_{0}$ w zbiorze testowym.

Równanie 2.7 mówi nam, że aby zminimalizować oczekiwany błąd testowy, musimy wybrać metodę uczenia statystycznego, która jednocześnie osiąga niską wariancję i niskie obciążenie. Zauważ, że wariancja jest z natury wielkością nieujemną, a kwadrat obciążenia również jest nieujemny. Stąd widzimy, że oczekiwany testowy MSE nigdy nie może spaść poniżej $Var(\epsilon)$, błędu nieredukowalnego z (2.3).

Co rozumiemy przez wariancję i obciążenie metody uczenia statystycznego? **Wariancja** odnosi się do tego, jak bardzo $\hat{f}$ by się zmieniło, gdybyśmy je estymowali, używając innego zbioru danych treningowych. Ponieważ dane treningowe są używane do dopasowania metody uczenia statystycznego, różne zbiory danych treningowych dadzą różne $\hat{f}$. Idealnie jednak estymacja f nie powinna zbytnio się różnić między zbiorami treningowymi. Jeśli jednak metoda ma wysoką wariancję, małe zmiany w danych treningowych mogą prowadzić do dużych zmian w $\hat{f}$. Ogólnie rzecz biorąc, bardziej elastyczne metody statystyczne mają wyższą wariancję. Rozważmy zielone i pomarańczowe krzywe na Rysunku 2.9. Elastyczna zielona krzywa bardzo dokładnie podąża za obserwacjami. Ma ona wysoką wariancję, ponieważ zmiana dowolnego z tych punktów danych może spowodować znaczną zmianę estymacji $\hat{f}$. W przeciwieństwie do tego, pomarańczowa linia regresji metodą najmniejszych kwadratów jest stosunkowo nieelastyczna i ma niską wariancję, ponieważ przesunięcie dowolnej pojedynczej obserwacji prawdopodobnie spowoduje tylko niewielką zmianę w położeniu linii.

Z drugiej strony, **obciążenie** odnosi się do błędu, który jest wprowadzany przez przybliżenie problemu z życia wziętego, który może być niezwykle skomplikowany, za pomocą znacznie prostszego modelu. Na przykład, regresja liniowa zakłada, że istnieje liniowa zależność między Y a $X_{1}, X_{2},..., X_{p}$. Jest mało prawdopodobne, że jakikolwiek problem z życia wziętego ma tak prostą liniową zależność, więc przeprowadzenie regresji liniowej niewątpliwie spowoduje pewne obciążenie w estymacji f. Na Rysunku 2.11 prawdziwe f jest znacznie nieliniowe, więc bez względu na to, ile obserwacji treningowych otrzymamy, nie będzie możliwe uzyskanie dokładnej estymacji przy użyciu regresji liniowej. Innymi słowy, regresja liniowa powoduje w tym przykładzie wysokie obciążenie. Jednak na Rysunku 2.10 prawdziwe f jest bardzo bliskie liniowemu, więc przy wystarczającej liczbie danych, regresja liniowa powinna być w stanie dać dokładną estymację. Generalnie, bardziej elastyczne metody dają mniejsze obciążenie.

Z reguły, im bardziej elastycznych metod używamy, tym wariancja będzie rosła, a obciążenie malało. Względna szybkość zmiany tych dwóch wielkości decyduje, czy testowy MSE rośnie, czy maleje. Gdy zwiększamy elastyczność klasy metod, obciążenie ma tendencję do początkowego spadku szybszego niż wzrost wariancji. W konsekwencji oczekiwany testowy MSE maleje. Jednak w pewnym momencie zwiększenie elastyczności ma niewielki wpływ na obciążenie, ale zaczyna znacznie zwiększać wariancję. Gdy to się dzieje, testowy MSE rośnie. Zauważ, że obserwowaliśmy ten wzorzec malejącego testowego MSE, a następnie rosnącego testowego MSE w prawych panelach Rysunków 2.9–2.11.

Trzy wykresy na Rysunku 2.12 ilustrują Równanie 2.7 dla przykładów z Rysunków 2.9–2.11. W każdym przypadku niebieska ciągła krzywa reprezentuje kwadrat obciążenia dla różnych poziomów elastyczności, podczas gdy pomarańczowa krzywa odpowiada wariancji. Pozioma linia przerywana reprezentuje Var(ϵ), czyli błąd nieredukowalny. Na koniec, czerwona krzywa, odpowiadająca testowemu MSE, jest sumą tych trzech wielkości. We wszystkich trzech przypadkach wariancja rośnie, a obciążenie maleje wraz ze wzrostem elastyczności metody. Jednakże poziom elastyczności odpowiadający optymalnemu testowemu MSE znacznie różni się między trzema zbiorami danych, ponieważ kwadrat obciążenia i wariancja zmieniają się w różnym tempie w każdym ze zbiorów danych. W lewym panelu Rysunku 2.12, obciążenie początkowo gwałtownie maleje, co skutkuje początkowym ostrym spadkiem oczekiwanego testowego MSE. Z drugiej strony, w środkowym panelu Rysunku 2.12, prawdziwe $f$ jest bliskie liniowemu, więc występuje tylko niewielki spadek obciążenia wraz ze wzrostem elastyczności, a testowy MSE tylko nieznacznie spada, zanim gwałtownie wzrośnie wraz ze wzrostem wariancji. Na koniec, w prawym panelu Rysunku 2.12, wraz ze wzrostem elastyczności, następuje gwałtowny spadek obciążenia, ponieważ prawdziwe $f$ jest bardzo nieliniowe. Występuje również bardzo niewielki wzrost wariancji wraz ze wzrostem elastyczności. W konsekwencji testowy MSE znacznie spada, zanim doświadczy niewielkiego wzrostu wraz ze wzrostem elastyczności modelu.

Zależność między obciążeniem, wariancją i testowym MSE, podana w Równaniu 2.7 i przedstawiona na Rysunku 2.12, jest nazywana **kompromisem między obciążeniem a wariancją**. Dobre wyniki na zbiorze testowym dla metody uczenia statystycznego wymagają niskiej wariancji oraz niskiego kwadratu obciążenia. Nazywa się to kompromisem, ponieważ łatwo jest uzyskać metodę o ekstremalnie niskim obciążeniu, ale wysokiej wariancji (na przykład rysując krzywą przechodzącą przez każdą pojedynczą obserwację treningową) lub metodę o bardzo niskiej wariancji, ale wysokim obciążeniu (dopasowując poziomą linię do danych). Wyzwaniem jest znalezienie metody, dla której zarówno wariancja, jak i kwadrat obciążenia są niskie. Ten kompromis jest jednym z najważniejszych powracających tematów w tej książce.

W rzeczywistej sytuacji, w której f jest nieobserwowane, generalnie nie jest możliwe jawne obliczenie testowego MSE, obciążenia ani wariancji dla metody uczenia statystycznego. Niemniej jednak, zawsze należy pamiętać o kompromisie między obciążeniem a wariancją. W tej książce badamy metody, które są niezwykle elastyczne i dlatego mogą w zasadzie wyeliminować obciążenie. Jednak nie gwarantuje to, że będą one lepsze od znacznie prostszej metody, takiej jak regresja liniowa. Aby podać skrajny przykład, załóżmy, że prawdziwe f jest liniowe. W tej sytuacji regresja liniowa nie będzie miała obciążenia, co sprawi, że bardziej elastycznej metodzie będzie bardzo trudno konkurować. W przeciwieństwie do tego, jeśli prawdziwe f jest wysoce nieliniowe i mamy dużą liczbę obserwacji treningowych, możemy uzyskać lepsze wyniki, używając bardzo elastycznego podejścia, jak na Rysunku 2.11. W Rozdziale 5 omawiamy walidację krzyżową, która jest sposobem na oszacowanie testowego MSE przy użyciu danych treningowych.

---

### 2.2.3 Ustawienie klasyfikacyjne

Dotychczas nasze omówienie dokładności modelu koncentrowało się na ustawieniu regresyjnym. Jednak wiele koncepcji, które napotkaliśmy, takich jak kompromis między obciążeniem a wariancją, przenosi się na ustawienie klasyfikacyjne z pewnymi modyfikacjami wynikającymi z faktu, że $y_{i}$ nie jest już ilościowe. Załóżmy, że chcemy estymować f na podstawie obserwacji treningowych $\{(x_{1}, y_{1}),..., (x_{n}, y_{n})\}$, gdzie teraz $y_{1},..., y_{n}$ są jakościowe. Najczęstszym podejściem do kwantyfikacji dokładności naszej estymacji $\hat{f}$ jest **wskaźnik błędu treningowego**, czyli proporcja błędów popełnionych, jeśli zastosujemy naszą estymację $\hat{f}$ do obserwacji treningowych:

$$\frac{1}{n}\sum_{i=1}^{n}I(y_{i} \neq \hat{y}_{i}). \quad (2.8)$$

Tutaj $\hat{y}_{i}$ jest przewidywaną etykietą klasy dla i-tej obserwacji przy użyciu $\hat{f}$. A $I(y_{i} \neq \hat{y}_{i})$ jest **zmienną wskaźnikową**, która jest równa 1, jeśli $y_{i} \neq \hat{y}_{i}$, i zero, jeśli $y_{i} = \hat{y}_{i}$. Jeśli $I(y_{i} \neq \hat{y}_{i}) = 0$, to i-ta obserwacja została poprawnie sklasyfikowana przez naszą metodę klasyfikacji; w przeciwnym razie została błędnie sklasyfikowana. Stąd Równanie 2.8 oblicza ułamek nieprawidłowych klasyfikacji.

Równanie 2.8 nazywane jest **wskaźnikiem błędu treningowego**, ponieważ jest obliczane na podstawie danych, które zostały użyte do wytrenowania naszego klasyfikatora. Podobnie jak w przypadku regresji, najbardziej interesują nas wskaźniki błędów, które wynikają z zastosowania naszego klasyfikatora do obserwacji testowych, które nie były używane w treningu. **Wskaźnik błędu testowego** związany ze zbiorem obserwacji testowych w postaci $(x_{0}, y_{0})$ jest dany przez

$$Ave(I(y_{0} \neq \hat{y}_{0})), \quad (2.9)$$

gdzie $\hat{y}_{0}$ jest przewidywaną etykietą klasy, która wynika z zastosowania klasyfikatora do obserwacji testowej z predyktorem $x_{0}$. Dobry klasyfikator to taki, dla którego błąd testowy (2.9) jest najmniejszy.

#### Klasyfikator Bayesa

Można wykazać (chociaż dowód wykracza poza zakres tej książki), że wskaźnik błędu testowego podany w (2.9) jest minimalizowany, średnio, przez bardzo prosty klasyfikator, który przypisuje każdą obserwację do najbardziej prawdopodobnej klasy, biorąc pod uwagę wartości jej predyktorów. Innymi słowy, powinniśmy po prostu przypisać obserwację testową z wektorem predyktorów $x_{0}$ do klasy j, dla której

$$Pr(Y = j|X = x_{0}) \quad (2.10)$$

jest największe. Zauważ, że (2.10) to **prawdopodobieństwo warunkowe**: jest to prawdopodobieństwo, że $Y = j$, biorąc pod uwagę obserwowany wektor predyktorów $x_{0}$. Ten bardzo prosty klasyfikator nazywa się **klasyfikatorem Bayesa**. W problemie dwuklasowym, gdzie są tylko dwie możliwe wartości odpowiedzi, powiedzmy klasa 1 lub klasa 2, klasyfikator Bayesa odpowiada przewidywaniu klasy pierwszej, jeśli $Pr(Y = 1|X = x_{0}) > 0.5$, a klasy drugiej w przeciwnym razie.

Rysunek 2.13 przedstawia przykład wykorzystujący symulowany zbiór danych w dwuwymiarowej przestrzeni składającej się z predyktorów $X_1$ i $X_2$. Pomarańczowe i niebieskie okręgi odpowiadają obserwacjom treningowym należącym do dwóch różnych klas. Dla każdej wartości $X_1$ i $X_2$ istnieje inne prawdopodobieństwo, że odpowiedź będzie pomarańczowa lub niebieska. Ponieważ są to dane symulowane, wiemy, jak zostały wygenerowane i możemy obliczyć prawdopodobieństwa warunkowe dla każdej wartości $X_1$ i $X_2$. Pomarańczowy zacieniowany region odzwierciedla zbiór punktów, dla których $Pr(Y = \text{pomarańczowy}\mid X)$ jest większe niż 50%, podczas gdy niebieski zacieniowany region wskazuje zbiór punktów, dla których prawdopodobieństwo jest poniżej 50%. Fioletowa linia przerywana reprezentuje punkty, w których prawdopodobieństwo wynosi dokładnie 50%. Nazywa się to **granicą decyzyjną Bayesa**. Predykcja klasyfikatora Bayesa jest określana przez granicę decyzyjną Bayesa; obserwacja, która znajdzie się po pomarańczowej stronie granicy, zostanie przypisana do klasy pomarańczowej, i podobnie, obserwacja po niebieskiej stronie granicy zostanie przypisana do klasy niebieskiej.

Klasyfikator Bayesa daje najniższy możliwy wskaźnik błędu testowego, nazywany **wskaźnikiem błędu Bayesa**. Ponieważ klasyfikator Bayesa zawsze wybierze klasę, dla której (2.10) jest największe, wskaźnik błędu wyniesie $1 - max_{j}Pr(Y = j|X = x_{0})$ przy $X = x_{0}$. Ogólnie rzecz biorąc, ogólny wskaźnik błędu Bayesa jest dany przez

$$1 - E(max_{j}Pr(Y = j|X)), \quad (2.11)$$

gdzie oczekiwanie uśrednia prawdopodobieństwo po wszystkich możliwych wartościach X. Dla naszych symulowanych danych, wskaźnik błędu Bayesa wynosi 0.133. Jest on większy od zera, ponieważ klasy nakładają się na siebie w prawdziwej populacji, co oznacza, że $max_{j}Pr(Y = j|X = x_{0}) < 1$ dla niektórych wartości $x_{0}$. Wskaźnik błędu Bayesa jest analogiczny do błędu nieredukowalnego, omówionego wcześniej.

#### K-najbliższych sąsiadów

W teorii zawsze chcielibyśmy przewidywać odpowiedzi jakościowe za pomocą klasyfikatora Bayesa. Ale dla rzeczywistych danych nie znamy rozkładu warunkowego Y przy danym X, więc obliczenie klasyfikatora Bayesa jest niemożliwe. Dlatego klasyfikator Bayesa służy jako nieosiągalny złoty standard, z którym porównuje się inne metody. Wiele podejść próbuje oszacować rozkład warunkowy Y przy danym X, a następnie klasyfikować daną obserwację do klasy o najwyższym oszacowanym prawdopodobieństwie. Jedną z takich metod jest klasyfikator **K-najbliższych sąsiadów (KNN)**. Dla danej dodatniej liczby całkowitej K i obserwacji testowej $x_{0}$, klasyfikator KNN najpierw identyfikuje K punktów w danych treningowych, które są najbliższe $x_{0}$, reprezentowane przez $\mathcal{N}_{0}$. Następnie szacuje prawdopodobieństwo warunkowe dla klasy j jako ułamek punktów w $\mathcal{N}_{0}$, których wartości odpowiedzi są równe j:

$$Pr(Y=j|X=x_{0}) = \frac{1}{K}\sum_{i \in \mathcal{N}_{0}}I(y_{i}=j). \quad (2.12)$$

Wreszcie, KNN klasyfikuje obserwację testową $x_{0}$ do klasy o największym prawdopodobieństwie z (2.12).

Rysunek 2.14 przedstawia ilustracyjny przykład podejścia KNN. W lewym panelu wykreśliliśmy mały treningowy zbiór danych składający się z sześciu niebieskich i sześciu pomarańczowych obserwacji. Naszym celem jest dokonanie predykcji dla punktu oznaczonego czarnym krzyżykiem. Załóżmy, że wybieramy $K = 3$. Wówczas KNN najpierw zidentyfikuje trzy obserwacje, które są najbliżej krzyżyka. To sąsiedztwo jest pokazane jako okrąg. Składa się ono z dwóch niebieskich punktów i jednego pomarańczowego, co skutkuje oszacowanymi prawdopodobieństwami 2/3 dla klasy niebieskiej i 1/3 dla klasy pomarańczowej. Dlatego KNN przewidzi, że czarny krzyżyk należy do klasy niebieskiej. W prawym panelu Rysunku 2.14 zastosowaliśmy podejście KNN z $K = 3$ dla wszystkich możliwych wartości $X_1$ i $X_2$ i narysowaliśmy odpowiadającą mu granicę decyzyjną KNN.

Mimo że jest to bardzo proste podejście, KNN często potrafi stworzyć klasyfikatory, które są zaskakująco bliskie optymalnemu klasyfikatorowi Bayesa. Rysunek 2.15 przedstawia granicę decyzyjną KNN, przy użyciu $K = 10$, zastosowaną do większego symulowanego zbioru danych z Rysunku 2.13. Zauważ, że mimo iż prawdziwy rozkład nie jest znany klasyfikatorowi KNN, granica decyzyjna KNN jest bardzo zbliżona do granicy klasyfikatora Bayesa. Wskaźnik błędu testowego przy użyciu KNN wynosi 0.1363, co jest bliskie wskaźnikowi błędu Bayesa wynoszącemu 0.1304.

Wybór K ma drastyczny wpływ na uzyskany klasyfikator KNN. Gdy $K = 1$, granica decyzyjna jest nadmiernie elastyczna i znajduje wzorce w danych, które nie odpowiadają granicy decyzyjnej Bayesa. Odpowiada to klasyfikatorowi o niskim obciążeniu, ale bardzo wysokiej wariancji. W miarę wzrostu K metoda staje się mniej elastyczna i tworzy granicę decyzyjną bliską liniowej. Odpowiada to klasyfikatorowi o niskiej wariancji, ale wysokim obciążeniu. Na tym symulowanym zbiorze danych ani $K = 1$, ani $K = 100$ nie dają dobrych prognoz: ich wskaźniki błędu testowego wynoszą odpowiednio 0.1695 i 0.1925.

Podobnie jak w przypadku regresji, nie ma silnego związku między wskaźnikiem błędu treningowego a wskaźnikiem błędu testowego. Przy $K = 1$, wskaźnik błędu treningowego KNN wynosi 0, ale wskaźnik błędu testowego może być dość wysoki. Ogólnie rzecz biorąc, im bardziej elastycznych metod klasyfikacji używamy, tym wskaźnik błędu treningowego będzie maleć, ale wskaźnik błędu testowego niekoniecznie. Na Rysunku 2.17 wykreśliliśmy błędy testowe i treningowe KNN jako funkcję $1/K$. W miarę wzrostu $1/K$ metoda staje się bardziej elastyczna. Podobnie jak w przypadku regresji, wskaźnik błędu treningowego konsekwentnie maleje wraz ze wzrostem elastyczności. Jednak błąd testowy wykazuje charakterystyczny kształt litery U, początkowo malejąc (z minimum przy około $K = 10$), a następnie ponownie rosnąc, gdy metoda staje się nadmiernie elastyczna i ulega przetrenowaniu.

Zarówno w przypadku regresji, jak i klasyfikacji, wybór odpowiedniego poziomu elastyczności jest kluczowy dla sukcesu każdej metody uczenia statystycznego. Kompromis między obciążeniem a wariancją oraz wynikający z niego kształt litery U w błędzie testowym mogą sprawić, że zadanie to będzie trudne. W Rozdziale 5 wrócimy do tego tematu i omówimy różne metody szacowania wskaźników błędu testowego, a tym samym wyboru optymalnego poziomu elastyczności dla danej metody uczenia statystycznego.