# Rozdział 12: Uczenie nienadzorowane

## 12.4 Metody klastrowe

### 12.4.2 Klastrowanie hierarchiczne

Jedną z potencjalnych wad klastrowania metodą $K$-średnich jest to, że wymaga ono wcześniejszego określenia liczby klastrów $K$. Klastrowanie hierarchiczne jest alternatywnym podejściem, które nie wymaga od nas zobowiązania się do konkretnego wyboru $K$. Klastrowanie hierarchiczne ma dodatkową zaletę w porównaniu z klastrowaniem $K$-średnich, ponieważ skutkuje atrakcyjną, drzewiastą reprezentacją obserwacji, zwaną **dendrogramem**.

W tej sekcji opisujemy klastrowanie **oddolne** lub **aglomeracyjne**. Jest to najczęstszy typ klastrowania hierarchicznego i odnosi się do faktu, że dendrogram (zazwyczaj przedstawiany jako odwrócone drzewo; patrz Rysunek 12.11) jest budowany, zaczynając od liści i łącząc klastry aż do pnia. Zaczniemy od omówienia, jak interpretować dendrogram, a następnie omówimy, jak faktycznie przeprowadzane jest klastrowanie hierarchiczne – czyli, jak budowany jest dendrogram.

**Interpretacja dendrogramu**

Zaczniemy od symulowanego zbioru danych przedstawionego na Rysunku 12.10, składającego się z 45 obserwacji w przestrzeni dwuwymiarowej. Dane zostały wygenerowane z modelu trójklasowego; prawdziwe etykiety klas dla każdej obserwacji są pokazane w różnych kolorach. Załóżmy jednak, że dane były obserwowane bez etykiet klas i że chcieliśmy przeprowadzić klastrowanie hierarchiczne danych. Klastrowanie hierarchiczne (z metodą pełnego łączenia, która zostanie omówiona później) daje wynik przedstawiony w lewym panelu Rysunku 12.11. Jak możemy zinterpretować ten dendrogram?

W lewym panelu Rysunku 12.11 każdy **liść** dendrogramu reprezentuje jedną z 45 obserwacji z Rysunku 12.10. Jednak w miarę przesuwania się w górę drzewa, niektóre liście zaczynają łączyć się w **gałęzie**. Odpowiadają one obserwacjom, które są do siebie podobne. W miarę przesuwania się wyżej w drzewie, same gałęzie łączą się, albo z liśćmi, albo z innymi gałęziami. Im wcześniej (niżej na drzewie) następują połączenia, tym bardziej podobne są do siebie grupy obserwacji. Z drugiej strony, obserwacje, które łączą się później (blisko szczytu drzewa), mogą być zupełnie inne. W rzeczywistości to stwierdzenie można sprecyzować: dla dowolnych dwóch obserwacji możemy poszukać punktu na drzewie, w którym gałęzie zawierające te dwie obserwacje po raz pierwszy się łączą. Wysokość tego połączenia, mierzona na osi pionowej, wskazuje, jak bardzo różne są te dwie obserwacje. Zatem obserwacje, które łączą się na samym dole drzewa, są do siebie bardzo podobne, podczas gdy obserwacje, które łączą się blisko szczytu drzewa, będą zwykle zupełnie inne.

To podkreśla bardzo ważny punkt w interpretacji dendrogramów, który jest często błędnie rozumiany. Rozważmy lewy panel Rysunku 12.12, który pokazuje prosty dendrogram uzyskany z hierarchicznego klastrowania dziewięciu obserwacji. Widać, że obserwacje 5 i 7 są do siebie bardzo podobne, ponieważ łączą się w najniższym punkcie dendrogramu. Obserwacje 1 i 6 są również do siebie bardzo podobne. Jednak kuszące, ale niepoprawne jest wnioskowanie z rysunku, że obserwacje 9 i 2 są do siebie bardzo podobne na podstawie tego, że znajdują się blisko siebie na dendrogramie. W rzeczywistości, na podstawie informacji zawartych w dendrogramie, obserwacja 9 nie jest bardziej podobna do obserwacji 2 niż do obserwacji 8, 5 i 7. (Można to zobaczyć na prawym panelu Rysunku 12.12, na którym wyświetlane są surowe dane.) Ujmując to matematycznie, istnieje $2^(n-1)$ możliwych zmian kolejności dendrogramu, gdzie $n$ to liczba liści. Dzieje się tak, ponieważ w każdym z $n-1$ punktów, w których dochodzi do połączeń, pozycje dwóch połączonych gałęzi można by zamienić, nie wpływając na znaczenie dendrogramu. Dlatego nie możemy wyciągać wniosków o podobieństwie dwóch obserwacji na podstawie ich bliskości na osi poziomej. Wnioski o podobieństwie dwóch obserwacji wyciągamy na podstawie lokalizacji na osi pionowej, gdzie gałęzie zawierające te dwie obserwacje po raz pierwszy się łączą.

Teraz, gdy rozumiemy, jak interpretować lewy panel Rysunku 12.11, możemy przejść do kwestii identyfikacji klastrów na podstawie dendrogramu. Aby to zrobić, wykonujemy poziome cięcie dendrogramu, jak pokazano w środkowym i prawym panelu Rysunku 12.11. Oddzielne zbiory obserwacji poniżej cięcia można zinterpretować jako klastry. W środkowym panelu Rysunku 12.11 przecięcie dendrogramu na wysokości dziewięciu skutkuje dwoma klastrami, pokazanymi w różnych kolorach. W prawym panelu przecięcie dendrogramu na wysokości pięciu skutkuje trzema klastrami. W miarę schodzenia w dół dendrogramu można dokonywać kolejnych cięć, aby uzyskać dowolną liczbę klastrów, od 1 (co odpowiada brakowi cięcia) do n (co odpowiada cięciu na wysokości 0, tak że każda obserwacja znajduje się w swoim własnym klastrze). Innymi słowy, wysokość cięcia dendrogramu pełni tę samą rolę co $K$ w klastrowaniu $K$-średnich: kontroluje liczbę uzyskanych klastrów.

Rysunek 12.11 podkreśla zatem bardzo atrakcyjny aspekt klastrowania hierarchicznego: jeden dendrogram można wykorzystać do uzyskania dowolnej liczby klastrów. W praktyce ludzie często patrzą na dendrogram i wybierają sensowną liczbę klastrów na oko, opierając się na wysokościach połączeń i pożądanej liczbie klastrów. W przypadku Rysunku 12.11 można by wybrać dwa lub trzy klastry. Jednak często wybór miejsca cięcia dendrogramu nie jest tak oczywisty.

Termin *hierarchiczny* odnosi się do faktu, że klastry uzyskane przez cięcie dendrogramu na danej wysokości są koniecznie zagnieżdżone w klastrach uzyskanych przez cięcie dendrogramu na dowolnej większej wysokości. Jednakże, dla dowolnego zbioru danych, to założenie o strukturze hierarchicznej może być nierealistyczne. Załóżmy na przykład, że nasze obserwacje odpowiadają grupie mężczyzn i kobiet, równo podzielonych na Amerykanów, Japończyków i Francuzów. Możemy sobie wyobrazić scenariusz, w którym najlepszy podział na dwie grupy mógłby podzielić te osoby według płci, a najlepszy podział na trzy grupy mógłby podzielić je według narodowości. W tym przypadku prawdziwe klastry nie są zagnieżdżone, w tym sensie, że najlepszy podział na trzy grupy nie wynika z wzięcia najlepszego podziału na dwie grupy i podzielenia jednej z tych grup. W konsekwencji taka sytuacja nie mogłaby być dobrze reprezentowana przez klastrowanie hierarchiczne. Z powodu takich sytuacji klastrowanie hierarchiczne może czasami dawać gorsze (tj. mniej dokładne) wyniki niż klastrowanie $K$-średnich dla danej liczby klastrów.

**Algorytm klastrowania hierarchicznego**

Dendrogram klastrowania hierarchicznego uzyskuje się za pomocą niezwykle prostego algorytmu. Zaczynamy od zdefiniowania pewnego rodzaju **miary braku podobieństwa** między każdą parą obserwacji. Najczęściej używana jest **odległość Euklidesowa**; omówimy wybór miary braku podobieństwa w dalszej części tego rozdziału. Algorytm postępuje iteracyjnie. Zaczynając od dołu dendrogramu, każda z n obserwacji jest traktowana jako osobny klaster. Dwa klastry, które są do siebie najbardziej podobne, są następnie łączone, tak że jest teraz $n-1$ klastrów. Następnie dwa klastry, które są do siebie najbardziej podobne, są ponownie łączone, tak że jest teraz n-2 klastrów. Algorytm postępuje w ten sposób, aż wszystkie obserwacje należą do jednego klastra, a dendrogram jest kompletny. Rysunek 12.13 przedstawia kilka pierwszych kroków algorytmu dla danych z Rysunku 12.12. Podsumowując, algorytm klastrowania hierarchicznego jest przedstawiony w Algorytmie 12.3.

**Algorytm 12.3 Klastrowanie hierarchiczne**
1. Zacznij od $n$ obserwacji i miary (takiej jak odległość Euklidesowa) wszystkich ${n\choose2} = n(n-1)/2$ parami braku podobieństwa. Potraktuj każdą obserwację jako osobny klaster.

2. Dla $i = n, n-1, ..., 2$:

    * (a) Zbadaj wszystkie parami braki podobieństwa między klastrami spośród $i$ klastrów i zidentyfikuj parę klastrów, które są najmniej różne (czyli najbardziej podobne). Połącz te dwa klastry. Brak podobieństwa między tymi dwoma klastrami wskazuje na wysokość w dendrogramie, na której powinno być umieszczone połączenie.

    * (b) Oblicz nowe parami braki podobieństwa między klastrami spośród $i-1$ pozostałych klastrów.

Ten algorytm wydaje się dość prosty, ale jedna kwestia nie została poruszona. Rozważmy prawy dolny panel na Rysunku 12.13. Jak ustaliliśmy, że klaster $\{5, 7\}$ powinien być połączony z klastrem $\{8\}$? Mamy pojęcie braku podobieństwa między parami obserwacji, ale jak zdefiniować brak podobieństwa między dwoma klastrami, jeśli jeden lub oba klastry zawierają wiele obserwacji? Pojęcie braku podobieństwa między parą obserwacji musi zostać rozszerzone na parę grup obserwacji. To rozszerzenie osiąga się poprzez rozwinięcie pojęcia **metody łączenia (linkage)**, która definiuje brak podobieństwa między dwiema grupami obserwacji. Cztery najczęstsze typy metod łączenia – pełne, średnie, pojedyncze i centroidowe – są krótko opisane w Tabeli 12.3. Metody łączenia średniego, pełnego i pojedynczego są najpopularniejsze wśród statystyków. Metody łączenia średniego i pełnego są generalnie preferowane nad pojedynczym, ponieważ zwykle dają bardziej zrównoważone dendrogramy. Metoda łączenia centroidowego jest często używana w genomice, ale ma poważną wadę polegającą na tym, że może wystąpić **inwersja**, w której dwa klastry są łączone na wysokości poniżej któregokolwiek z pojedynczych klastrów w dendrogramie. Może to prowadzić do trudności w wizualizacji, a także w interpretacji dendrogramu. Braki podobieństwa obliczone w Kroku 2(b) algorytmu klastrowania hierarchicznego będą zależeć od rodzaju użytej metody łączenia, a także od wyboru miary braku podobieństwa. Stąd wynikowy dendrogram zazwyczaj bardzo silnie zależy od rodzaju użytej metody łączenia, jak pokazano na Rysunku 12.14.

**Tabela 12.3: Podsumowanie czterech najczęściej używanych typów metod łączenia w klastrowaniu hierarchicznym.**

| Metoda łączenia | Opis |
| :--- | :--- |
| **Pełne** | Maksymalny brak podobieństwa między klastrami. Oblicz wszystkie parami braki podobieństwa między obserwacjami w klastrze A i obserwacjami w klastrze B, i zapisz największy z tych braków podobieństwa. |
| **Pojedyncze** | Minimalny brak podobieństwa między klastrami. Oblicz wszystkie parami braki podobieństwa między obserwacjami w klastrze A i obserwacjami w klastrze B, i zapisz najmniejszy z tych braków podobieństwa. Metoda pojedynczego łączenia może skutkować wydłużonymi, ciągnącymi się klastrami, w których pojedyncze obserwacje są łączone jedna po drugiej. |
| **Średnie** | Średni brak podobieństwa między klastrami. Oblicz wszystkie parami braki podobieństwa między obserwacjami w klastrze A i obserwacjami w klastrze B, i zapisz średnią tych braków podobieństwa. |
| **Centroidowe** | Brak podobieństwa między centroidem klastra A (wektor średnich o długości p) a centroidem klastra B. Metoda łączenia centroidowego może skutkować niepożądanymi inwersjami. |

**Wybór miary braku podobieństwa**

Do tej pory przykłady w tym rozdziale wykorzystywały odległość Euklidesową jako miarę braku podobieństwa. Ale czasami preferowane mogą być inne miary braku podobieństwa. Na przykład, **odległość oparta na korelacji** uważa dwie obserwacje za podobne, jeśli ich cechy są silnie skorelowane, nawet jeśli obserwowane wartości mogą być odległe pod względem odległości Euklidesowej. Jest to nietypowe użycie korelacji, która zwykle jest obliczana między zmiennymi; tutaj jest obliczana między profilami obserwacji dla każdej pary obserwacji. Rysunek 12.15 ilustruje różnicę między odległością Euklidesową a odległością opartą na korelacji. Odległość oparta na korelacji skupia się na kształtach profili obserwacji, a nie na ich wielkościach.

Wybór miary braku podobieństwa jest bardzo ważny, ponieważ ma silny wpływ na wynikowy dendrogram. Ogólnie rzecz biorąc, należy zwrócić szczególną uwagę na rodzaj klastrowanych danych i pytanie naukowe. Te rozważania powinny determinować, jaki rodzaj miary braku podobieństwa jest używany w klastrowaniu hierarchicznym.

Rozważmy na przykład sprzedawcę internetowego zainteresowanego klastrowaniem kupujących na podstawie ich wcześniejszych historii zakupów. Celem jest zidentyfikowanie podgrup podobnych kupujących, tak aby kupującym w każdej podgrupie można było pokazywać przedmioty i reklamy, które szczególnie mogą ich zainteresować. Załóżmy, że dane mają postać macierzy, w której wiersze to kupujący, a kolumny to przedmioty dostępne do zakupu; elementy macierzy danych wskazują, ile razy dany kupujący zakupił dany przedmiot (tj. 0, jeśli kupujący nigdy nie kupił tego przedmiotu, 1, jeśli kupił go raz, itd.). Jaki rodzaj miary braku podobieństwa należy użyć do klastrowania kupujących? Jeśli użyta zostanie odległość Euklidesowa, kupujący, którzy kupili bardzo mało przedmiotów ogółem (tj. rzadcy użytkownicy witryny zakupów online), zostaną sklastrowani razem. To może nie być pożądane. Z drugiej strony, jeśli użyta zostanie odległość oparta na korelacji, kupujący o podobnych preferencjach (np. kupujący, którzy kupili przedmioty A i B, ale nigdy przedmiotów C lub D) zostaną sklastrowani razem, nawet jeśli niektórzy kupujący o tych preferencjach robią zakupy o większej objętości niż inni. Dlatego w tym zastosowaniu odległość oparta na korelacji może być lepszym wyborem.

Oprócz starannego wyboru użytej miary braku podobieństwa, należy również rozważyć, czy zmienne powinny być skalowane, aby miały odchylenie standardowe równe jeden, zanim zostanie obliczony brak podobieństwa między obserwacjami. Aby zilustrować ten punkt, kontynuujemy właśnie opisany przykład zakupów online. Niektóre przedmioty mogą być kupowane częściej niż inne; na przykład kupujący może kupować dziesięć par skarpet rocznie, ale komputer bardzo rzadko. Zakupy o wysokiej częstotliwości, takie jak skarpety, mają zatem tendencję do znacznie większego wpływu na braki podobieństwa między kupującymi, a co za tym idzie na ostatecznie uzyskane klastrowanie, niż rzadkie zakupy, takie jak komputery. To może nie być pożądane. Jeśli zmienne zostaną przeskalowane, aby miały odchylenie standardowe równe jeden, zanim zostaną obliczone braki podobieństwa między obserwacjami, każda zmienna będzie miała w efekcie równe znaczenie w przeprowadzanym klastrowaniu hierarchicznym. Możemy również chcieć przeskalować zmienne, aby miały odchylenie standardowe równe jeden, jeśli są mierzone w różnych skalach; w przeciwnym razie wybór jednostek (np. centymetry w porównaniu z kilometrami) dla danej zmiennej znacznie wpłynie na uzyskaną miarę braku podobieństwa. Nie powinno dziwić, że to, czy skalowanie zmiennych przed obliczeniem miary braku podobieństwa jest dobrą decyzją, zależy od danego zastosowania. Przykład pokazano na Rysunku 12.16. Zauważmy, że kwestia tego, czy skalować zmienne przed wykonaniem klastrowania, dotyczy również klastrowania $K$-średnich.